{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex: Explanation of the Numpy version of our Gradient Descent function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the gradient descent function we have been taking advantage of Numpy.\n",
    "\n",
    "We use Numpy as this way we can use linear algebra to make calculations faster and the function more generic, as we can then use matrices of any size and the function will respond with the right number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def GradientDescent_np(X, y, max_iterations=100, alpha=1):\n",
    "    m = X.shape[0] # number of samples\n",
    "    J = np.zeros(max_iterations)\n",
    "\n",
    "    # y must be a column vector of shape m x 1\n",
    "    y = y.reshape(m, 1)\n",
    "    \n",
    "    #initialize the parameters to zero\n",
    "    w = np.zeros(shape=(X.shape[1], 1))\n",
    "    \n",
    "    # Repeat for max_iterations (it would be nice to also check convergence...)\n",
    "    for iteration in range(max_iterations):\n",
    "        grad = np.dot(X.T , (np.dot(X,w) - y)) / m\n",
    "        w = w - alpha*grad\n",
    "        J[iteration] = sum( (np.dot(X,w) - y)**2)/m\n",
    "    return [w, J]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me take some time to explain you the line where we calcualte the gradients, as it might not be very obvious how we do this by multiplying matrices. I am talking about the line:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m`\n",
    "\n",
    "Where `X` is the input (the Design Matrix) `y` is the vector column of the output, and theta is a vector column of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix `X` for a dataset of of $m$ samples (data points) and $n$ features, should have $m$ rows and $n+1$ columns, and looks as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Each row corresponds to a data point, and each column corresponds to a feature.\n",
    "\n",
    "Our outputs (labels, correct values, ground truth) `y` is a column vector of as many rows as data points:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The number of parameters would be equal to the number of features plus the bias (corresponding to the ficticious feature $x_0$, and we can write it as a vector `w` as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "w = \n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots   \\\\\n",
    "w_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to calculate is the gradient (partial derivatives) for all thetas from $w_0$ to $w_n$. These will be used afterwards to change each of the thetas accordingly. These are given by \n",
    "\n",
    "$\\frac{\\partial}{\\partial w_0} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_0^{(i)}}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_1} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_1^{(i)}}$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_n} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_n^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we would like at each iteration is to calculate a list of partial derivatives (gradients). This is what our vector `grad` will contain at the end of the operation. And we will do it in a single line of code, taking advantage of the linear algebra capabilities of NumPy:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start explaining from the inside out. First let's see what the part `np.dot(X, w)` does. This is a multiplication of the matrix `X` with the vector `w` and it gives us the $f_w$ for each of the points in our dataset.\n",
    "\n",
    "\\begin{equation*}\n",
    "X w = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots   \\\\\n",
    "w_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_w^{(1)} \\\\\n",
    "f_w^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_w^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the part of `np.dot(X, w) - y` is basically the residuals, the differences between what we calculate and what the right value should be FOR EACH of the points of our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Residuals = \n",
    "\\begin{bmatrix}\n",
    "f_w^{(1)} \\\\\n",
    "f_w^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_w^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_w^{(1)} - y^{(1)}\\\\\n",
    "f_w^{(2)} - y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_w^{(m)} - y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to calculate the gradient corresponding to a specific parameter, we need to multiply each of these residuals with the corresponding feature, and then sum all these multipied residues over all the points of our dataset. This is what the other dot product does: `np.dot(X.T , (np.dot(X, w) - y))`, it transposes the Design Matrix and then multiplies it with our vector of residues. Let's see what this does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "X^T Residues = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_0^{(2)} & \\cdots & x_0^{(m)} \\\\\n",
    "x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f_w^{(1)} - y^{(1)}\\\\\n",
    "f_w^{(2)} - y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_w^{(m)} - y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(f_w^{(1)} - y^{(1)}) x_0^{(1)} + (f_w^{(2)} - y^{(2)}) x_0^{(2)} + \\cdots + (f_w^{(m)} - y^{(m)}) x_0^{(m)} \\\\\n",
    "(f_w^{(1)} - y^{(1)}) x_1^{(1)} + (f_w^{(2)} - y^{(2)}) x_1^{(2)} + \\cdots + (f_w^{(m)} - y^{(m)}) x_1^{(m)} \\\\\n",
    "\\vdots \\\\\n",
    "(f_w^{(1)} - y^{(1)}) x_1^{(m)} + (f_w^{(2)} - y^{(2)}) x_n^{(2)} + \\cdots + (f_w^{(m)} - y^{(m)}) x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_0^{(i)}} \\\\\n",
    "\\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_1^{(i)}} \\\\\n",
    "\\vdots  \\\\\n",
    "\\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_n^{(i)}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing remaining is to divide everything with m, so the final result of this line is a column vector with the following contents:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "grad = \n",
    "\\begin{bmatrix}\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_0^{(i)}} \\\\\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_1^{(i)}} \\\\\n",
    "\\vdots  \\\\\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_W(x^{(i)}) - y^{(i)}) x_n^{(i)}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
