{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef509804",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dkaratzas/ML2024-25/blob/main/Session%2010%20-%20EM/P10_EM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940f2c3",
   "metadata": {},
   "source": [
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/dkaratzas/ML2024-25/blob/main/Session%2010%20-%20EM/P10_EM.ipynb>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee456ac",
   "metadata": {},
   "source": [
    "# Problems 10 - Expectation Maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbe4dd",
   "metadata": {},
   "source": [
    "In this notebook we will define our own version of a Gaussian Mixture Model and use it to represent distributions and to create new samples out of them.\n",
    "\n",
    "Then we will implement our own version of the Expectation Maximisation algorithm, and compare it with the sklearn implementation. We will take advantage of all the action to explore a couple of ways to visualise data in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c667c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbb509",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10.1 - Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861138b",
   "metadata": {},
   "source": [
    "A Gaussian Mixture Model is basically a weighted sum of Normal (Gaussian) distributions:\n",
    "\n",
    "$$𝑝(𝐱)=\\sum_{k=1}^{N}\\pi_kN(x\\mid\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "where $\\sum_{k=1}^{N}\\pi_k=1$.\n",
    "\n",
    "To manage this, we will create a class that will basically act as a collection of Normal distributions. We will provide the class means $\\mu_k$ and covariances $\\Sigma_k$, as well as the mixing coefficients $\\pi_k$. Below is the code for this class. Go over it and try to understand what each function is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68065e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, means, covs, coefs):\n",
    "        \"\"\"Class initilization.\n",
    "\n",
    "        Keyword arguments:\n",
    "        means -- a list of mean vectors, one for each component\n",
    "        covs  -- a list of covariance matrices, one for each component\n",
    "        coefs -- a list of mixing coefficients\n",
    "        \n",
    "        The function checks that the number of means, covariance matrices \n",
    "        and mixing coefficients agree. Otherwise it raises an exception.\n",
    "        It normalises the coeficient values (so they do not have to sum to\n",
    "        1 when specified), but they have to be all positive numbers.\n",
    "        \"\"\"        \n",
    "        assert (means.ndim == 2)\n",
    "        self.nComponents, self.dim = means.shape\n",
    "\n",
    "        assert (coefs.ndim == 1)\n",
    "        assert (means.shape[0] == coefs.shape[0] == self.nComponents)\n",
    "        assert (covs.shape[1] == covs.shape[2] == self.dim)\n",
    "        assert all(coefs > 0)\n",
    "        self.means = means\n",
    "        self.covs = covs\n",
    "        self.coefs = coefs / coefs.sum() # Make sure these are valid probabilities\n",
    "                           \n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class.        \n",
    "        Returns an informative string about our class.\n",
    "        \"\"\"\n",
    "        return f\"GMM with {self.nComponents} components\"\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Implements the call operator. This enables instances of our class to\n",
    "        behave like functions and be called like a function.        \n",
    "        It returns the probability density value at x. For this it calls the pdf() \n",
    "        function that is an alternative way to do that.\n",
    "        \"\"\"\n",
    "        return self.pdf(x)\n",
    "        \n",
    "    def pdf(self, x):\n",
    "        \"\"\"Returns the probability density value at x.\n",
    "        \"\"\"\n",
    "        out = 0        \n",
    "        for mean, cov, coef in zip(self.means, self.covs, self.coefs):\n",
    "            out += coef * multivariate_normal(mean, cov).pdf(x)\n",
    "        return out\n",
    "    \n",
    "    def sample(self, size = None):        \n",
    "        \"\"\"Returns a set of feature vector, according to the size specified, sampled from the GMM.\n",
    "        The final result will be of size (size, dim), where dim is the dimensionality of the feature vector\n",
    "        \"\"\"\n",
    "        # Select random components, as many as \"size\" indicates\n",
    "        c = np.random.choice(np.arange(self.nComponents), size=size, replace=True, p=self.coefs)\n",
    "        \n",
    "        # c will have the size the user specified, make it a list of indices\n",
    "        clist = c.reshape(-1, 1).squeeze()\n",
    "        \n",
    "        # Check in case we only got a single number (not an array)\n",
    "        if clist.ndim == 0: # this would happen if the user asked for only one element (dimensionality = 0), then expand it so that the rest works\n",
    "            clist = np.expand_dims(clist, 0)\n",
    "        \n",
    "        # create the output array, as an array of size like the list of indices\n",
    "        out = np.zeros( (clist.shape[0], self.dim) )\n",
    "        \n",
    "        # Generate samples from the corresponding clusters\n",
    "        for i, cl in enumerate(clist):\n",
    "            out[i] = np.random.multivariate_normal(self.means[cl], self.covs[cl])\n",
    "            \n",
    "        # Reshape out back to the desired size        \n",
    "        out = out.reshape(c.shape + (self.dim,))\n",
    "        #out = out.reshape(np.hstack( (c.shape, self.dim)).astype(int) )  # Alternative way to do this\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73002b",
   "metadata": {},
   "source": [
    "Let´s first have a look at the `GMM.pdf()` fuction of this class. This class returns the probability of our GMM at some input point $x$. To understand how it works, check the documentation of the function `scipy.stats.multivariate_normal()` here:\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd3962",
   "metadata": {},
   "source": [
    "To test our function, we will manually define a GMM with two components, then use the `GMM.pdf()` function to calculate the probability density at some point $x$. We will work in a 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e677df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_spd_matrix\n",
    "\n",
    "rng = 12 # range to use for sampling\n",
    "\n",
    "nComponents = 2\n",
    "means = np.random.uniform(-rng+2, rng-2, size = (nComponents, 2))\n",
    "covs = np.array( [make_spd_matrix(n_dim=2, random_state=r) for r in np.random.randint(100, size = nComponents)] )\n",
    "coefs = np.random.uniform(1, 5, nComponents)\n",
    "coefs /= nComponents # mixing coefficients, normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(means, covs, coefs)\n",
    "x = np.array([0, 0])\n",
    "print(gmm.pdf(x))\n",
    "print(gmm(x))\n",
    "\n",
    "print(gmm(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1bcbd",
   "metadata": {},
   "source": [
    "Now have a look at the `GMM.sample()` function of our class. This function generates samples from our GMM distribution. To understand how it works, check the documentation of the function `random.multivariate_normal()` here:\n",
    "https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html\n",
    "\n",
    "And the function `numpy.random.choice()` here:\n",
    "https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49e18f",
   "metadata": {},
   "source": [
    "Here are some tests of our sampling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 - generate a single sample\n",
    "gmm.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 - generate a set of 5 samples\n",
    "gmm.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 - generate a set of 5 samples (alternative way)\n",
    "gmm.sample([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 - generate an array of samples of the size given\n",
    "gmm.sample([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some plotting\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "\n",
    "data = gmm.sample(1000) # generate 1000 samples\n",
    "plt.scatter(data[:,0], data[:,1], marker = \".\")\n",
    "plt.plot(means[:,0], means[:,1], \"kx\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(-rng, rng)\n",
    "plt.ylim(-rng, rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44faf28c",
   "metadata": {},
   "source": [
    "Each one probably got a different result, as we randomly generated means, covariances and samples. We will now formalise our initialisation procedure, to create random means, covariances, and mixing coefficients in the same way, by controlling the seed we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91dde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "\n",
    "def initialise(nComponents, rng = 10, random_seed = 6):\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    coefs = np.ones(nComponents)/nComponents # mixing coefficients\n",
    "\n",
    "    means = np.random.uniform(-rng+2, rng-2, size = (nComponents, 2))\n",
    "    covs = np.array( [make_spd_matrix(n_dim=2, random_state=r) for r in np.random.randint(100, size = nComponents)] )\n",
    "    \n",
    "    return means, covs, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30f69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 2\n",
    "means, covs, coefs = initialise(nComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(means, covs, coefs)\n",
    "print(gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some plotting\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "\n",
    "data = gmm.sample(1000)\n",
    "plt.scatter(data[:,0], data[:,1], marker = \".\")\n",
    "plt.plot(means[:,0], means[:,1], \"kx\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(-rng, rng)\n",
    "plt.ylim(-rng, rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ab0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm( [0, 0] ) # The probability density function at [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de4acc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10.2 - Expectation Maximisation\n",
    "\n",
    "Here we will use our our implementation of Expectation Maximisation.\n",
    "\n",
    "We will initialise with some random clusters, and apply EM on the data they have provided us. First we will load and explore the data we have been given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b87344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf40162",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data.pklz', 'rb') as f:\n",
    "    (X, y) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc090b66",
   "metadata": {},
   "source": [
    "Let's first explore a bit and visualise the data we just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature dimension: \", X.ndim)\n",
    "for i, column in enumerate(X.T):\n",
    "    print(\"Feature {index} in [{minimum:.2f}, {maximum:.2f}]\".format(index = i, minimum = column.min(), maximum = column.max()))\n",
    "    \n",
    "for label in np.unique(y):\n",
    "    print(\"Label {} count: {} ({:.2f}%)\".format(label, np.sum(y == label), 100*np.sum(y == label)/len(y) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8, 8))\n",
    "scatter = plt.scatter(X[:,0], X[:,1], c = y, marker = \"o\", alpha = 0.5, cmap=\"viridis\", label = y, s=150, edgecolors=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(-rng, rng)\n",
    "plt.ylim(-rng, rng)\n",
    "plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cdedd",
   "metadata": {},
   "source": [
    "Now for the implementation of our Expectation Maximisation function. We will start with a number of random components. Then in every iteration (epoch) we will calculate:\n",
    "\n",
    "**EXPECTATION STEP:**\n",
    "\n",
    "(1) the \"responsibilities\" of each component for each point of our dataset:\n",
    "\n",
    "$$\\gamma(z_{nk})=\\frac{\\pi_k N(x^{(n)}\\mid\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K\\pi_j N(x^{(n)}\\mid\\mu_j, \\Sigma_j)} $$\n",
    "\n",
    "**MAXIMISATION STEP:**\n",
    "\n",
    "(2) the new means:\n",
    "\n",
    "$$\\mu_k^{new}=\\frac{1}{N_k}\\sum_{n=1}^N\\gamma(z_{nk})x^{(n)}$$\n",
    "\n",
    "(3) the new covariance matrices:\n",
    "\n",
    "$$\\Sigma_k^{new}=\\frac{1}{N_k}\\sum_{n=1}^N\\gamma(z_{nk})(x^{(n)}-\\mu_k^{new})(x^{(n)}-\\mu_k^{new})^T$$\n",
    "\n",
    "where\n",
    "\n",
    "$$N_k=\\sum_{n+1}^N\\gamma(z_{nk})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a90af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def em(X, means, covs, coefs, nIter = 100, resetSingulars = False, epsilon = 1e-6):\n",
    "    '''Implements the Expectation - Maximisation algorithm for estimating probability density using a Gaussian Mixture Model\n",
    "    means          - The initial means to use\n",
    "    covs           - The initial covariance matrices to use\n",
    "    coefs          - The initial mixture coefficients to use\n",
    "    nIter          - Number of epochs to do\n",
    "    resetSingulars - If True, it checks for singular compoments and resets them to a new position. If False, \n",
    "                     it avoids singular components by adding a small epsilon value on the diagonal of all covariance matrices\n",
    "    epsilon        - The epsilon to use if resetSingulars is set to False\n",
    "    '''\n",
    "    for epochs in range(nIter):\n",
    "        # (E-Step)\n",
    "        # Calculate the responsibilities\n",
    "        gamma = np.array([z * multivariate_normal(m, c).pdf(X) for m, c, z in zip(means, covs, coefs)])\n",
    "        gamma /= gamma.sum(axis=0)\n",
    "\n",
    "        # (M-Step)\n",
    "        # Calculate the Ns (the sum of this vector should equal the total number of points)\n",
    "        N = gamma.sum(axis = 1)\n",
    "\n",
    "        # Calculate means\n",
    "        means = np.array([(gamma[i,:] * X.T).sum(axis=1)/n for i, n in enumerate(N)])\n",
    "\n",
    "        # Calculate new coefficients\n",
    "        coefs = N/sum(N)\n",
    "\n",
    "        # Calculate covariances\n",
    "        covs = np.array([(gamma[i,:] * (X - m).T) @ (X - m)/n for i, (m, n), in enumerate(zip(means, N))])    \n",
    "\n",
    "        # Deal with any singular components, we offer two ways\n",
    "        if resetSingulars:\n",
    "            # Check if any component has become singular and reset it to a random place\n",
    "            nDims = means.shape[1]\n",
    "            if any(np.linalg.matrix_rank(covs) < nDims):\n",
    "                print(\"Singular covariance found, resetting component\")\n",
    "                idx = (np.linalg.matrix_rank(covs) < nDims)\n",
    "                nSingulars = idx.sum()\n",
    "                means[idx] = np.random.uniform(-rng+2, rng-2, size = (nSingulars, 2))\n",
    "                covs[idx] = np.array( [make_spd_matrix(n_dim=nDims, random_state=r) for r in np.random.randint(100, size = nSingulars)] )\n",
    "                coefs[idx] = (1/coefs.shape[0])\n",
    "                coefs = coefs/sum(coefs)\n",
    "        else:\n",
    "            # Or alternatively, do not reset any component, just avoid singularities\n",
    "            covs += epsilon * np.identity(covs.shape[1]) # Avoid singular matrices\n",
    "            \n",
    "    return means, covs, coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb63ca1",
   "metadata": {},
   "source": [
    "Set some initial values for the components, and run the EM algorithm on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ea1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 2\n",
    "means, covs, coefs = initialise(nComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f61a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means, pred_covs, pred_coefs = em(X, means, covs, coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddb7f5",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Create a GMM using the predicted means, covariance matrices, and mixing coeficients and generate 500 samples. Then plot the generated points - do they make sense? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a8d51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b9661",
   "metadata": {},
   "source": [
    "Here's a function to plot contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "797b4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(f, data, means = None, labels = None, xlim = (-12, 12), ylim = (-12, 12), resolution = 100):\n",
    "    '''Plots contours, given a function f\n",
    "    f          - reference to a function that accepts 2-dimensional points as input. E.g. f([x, y])\n",
    "    data       - a list of data points. Rows are points, columns are features. 2-dimensional points are expected (extra dimensions are ignored)\n",
    "    labels     - if given, they will be used to colour the points\n",
    "    xlim       - the x-limits of the plot\n",
    "    ylim       - the y-limits of the plot\n",
    "    resolution - the sampling resolution to use to calculate the contours\n",
    "    '''\n",
    "    XX = np.linspace(xlim[0], xlim[1], resolution)\n",
    "    YY = np.linspace(ylim[0], ylim[1], resolution)\n",
    "\n",
    "    XX, YY = np.meshgrid(XX, YY)\n",
    "\n",
    "    ZZ = f( np.hstack( (XX.reshape(-1, 1), YY.reshape(-1, 1)) ) )\n",
    "    ZZ = ZZ.reshape(XX.shape)\n",
    "    \n",
    "    fig = plt.figure(figsize = (5, 5), dpi = 150)\n",
    "\n",
    "    plt.xlim(xlim[0], xlim[1])\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "\n",
    "    cm = plt.cm.Set1\n",
    "    plt.scatter(data[:,0], data[:,1], color = \"grey\" if (labels is None) else cm(labels), marker = \".\")\n",
    "\n",
    "    cm = plt.cm.coolwarm\n",
    "    levels = np.linspace(0.0001, min(0.04, np.max(ZZ)), 7)\n",
    "    plt.contour(XX, YY, ZZ, cmap = cm, levels = levels)\n",
    "    \n",
    "    if means is not None:\n",
    "        plt.plot(means[:, 0], means[:, 1], \"kx\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b77353",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(gmm, X, pred_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aabb5a",
   "metadata": {},
   "source": [
    "Just to do a quick check, we calculate the volume under surface in the region plotted (should be close to 1, as most of the action happens here, but the GMM extends to infinity, so there is a part of the volume outside the shown region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35ff1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "\n",
    "def trapezoidal_area(xyz):\n",
    "    \"\"\"Calculate volume under a surface defined by irregularly spaced points\n",
    "    using delaunay triangulation. \"x,y,z\" is a <numpoints x 3> shaped ndarray.\"\"\"\n",
    "    d = scipy.spatial.Delaunay(xyz[:,:2])\n",
    "    tri = xyz[d.simplices]\n",
    "\n",
    "    a = tri[:,0,:2] - tri[:,1,:2]\n",
    "    b = tri[:,0,:2] - tri[:,2,:2]\n",
    "    vol = np.cross(a, b) @ tri[:,:,2]\n",
    "    return vol.sum() / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.linspace(-12, 12, 100)\n",
    "YY = np.linspace(-12, 12, 100)\n",
    "XX, YY = np.meshgrid(XX, YY)\n",
    "ZZ = gmm( np.hstack( (XX.reshape(-1, 1), YY.reshape(-1, 1)) ) )\n",
    "\n",
    "xyz = np.hstack( (XX.reshape(-1, 1), YY.reshape(-1, 1), ZZ.reshape(-1, 1)))\n",
    "\n",
    "trapezoidal_area(xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f78720",
   "metadata": {},
   "source": [
    "Here are a couple of more plots, to help with your exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "503f33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(f, data, labels = None, xlim = (-12, 12), ylim = (-12, 12), resolution = 100):\n",
    "    '''Plots a surface plot, given a function f\n",
    "    f          - reference to a function that accepts 2-dimensional points as input. E.g. f([x, y])\n",
    "    data       - a list of data points. Rows are points, columns are features. 2-dimensional points are expected (extra dimensions are ignored)\n",
    "    labels     - if given, they will be used to colour the points\n",
    "    xlim       - the x-limits of the plot\n",
    "    ylim       - the y-limits of the plot\n",
    "    resolution - the sampling resolution to use to calculate the contours\n",
    "    '''\n",
    "    XX = np.linspace(xlim[0], xlim[1], resolution)\n",
    "    YY = np.linspace(ylim[0], ylim[1], resolution)\n",
    "\n",
    "    XX, YY = np.meshgrid(XX, YY)\n",
    "\n",
    "    ZZ = f( np.hstack( (XX.reshape(-1, 1), YY.reshape(-1, 1)) ) )\n",
    "    ZZ = ZZ.reshape(XX.shape)\n",
    "    \n",
    "    fig = plt.figure(figsize = (5, 5), dpi = 150)\n",
    "    ax = fig.add_subplot(projection = \"3d\")\n",
    "\n",
    "    zl = 0.04 # The z limit we will use\n",
    "\n",
    "    # Plot the 3D surface    \n",
    "    ax.plot_surface(XX, YY, ZZ, edgecolor='royalblue', lw=0.2, rstride=2, cstride=2, alpha=0.3,  cmap = 'coolwarm')\n",
    "    \n",
    "    # Set axis limits, labels, ticks\n",
    "    ax.set(xlim=xlim, ylim=ylim, zlim=(0, zl), xlabel='$x_1$', ylabel='$x_2$', zlabel='$p(x)$')\n",
    "    ax.set_xticks(np.arange(xlim[0], xlim[1]+1, 5) )\n",
    "    ax.set_yticks(np.arange(xlim[0], xlim[1]+1, 5) )\n",
    "\n",
    "    # Rotate the graph    \n",
    "    ax.view_init(elev = 25, azim = -70)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surface(gmm, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f89a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined(f, data, labels = None, xlim = (-12, 12), ylim = (-12, 12), resolution = 100):\n",
    "    '''Plots a combined contour and surface plot, given a function f\n",
    "    f          - reference to a function that accepts 2-dimensional points as input. E.g. f([x, y])\n",
    "    data       - a list of data points. Rows are points, columns are features. 2-dimensional points are expected (extra dimensions are ignored)\n",
    "    labels     - if given, they will be used to colour the points\n",
    "    xlim       - the x-limits of the plot\n",
    "    ylim       - the y-limits of the plot\n",
    "    resolution - the sampling resolution to use to calculate the contours\n",
    "    '''\n",
    "    XX = np.linspace(xlim[0], xlim[1], resolution)\n",
    "    YY = np.linspace(ylim[0], ylim[1], resolution)\n",
    "\n",
    "    XX, YY = np.meshgrid(XX, YY)\n",
    "\n",
    "    ZZ = f( np.hstack( (XX.reshape(-1, 1), YY.reshape(-1, 1)) ) )\n",
    "    ZZ = ZZ.reshape(XX.shape)\n",
    "    \n",
    "    fig = plt.figure(figsize = (5, 5), dpi = 150)\n",
    "    ax = fig.add_subplot(projection = \"3d\")\n",
    "\n",
    "    zl = 0.04 # The z limit we will use\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    ax.plot_surface(XX, YY, ZZ, edgecolor='royalblue', lw=0.2, rstride=1, cstride=1, alpha=0.3,  cmap = 'coolwarm')\n",
    "\n",
    "    # Plot a projection of the contour\n",
    "    ax.contourf(XX, YY, ZZ, zdir='z', offset=-2*zl, cmap='coolwarm')\n",
    "\n",
    "    # Plot the data\n",
    "    ax.scatter(X[:,0], X[:,1], -2*zl, c = \"black\", marker = \"+\", s= 10, linewidth = 0.5)\n",
    "\n",
    "    # Rotate the graph\n",
    "    ax.view_init(elev = 20, azim = -60)\n",
    "\n",
    "    # Set axis limits, labels, ticks\n",
    "    ax.set(xlim=(-rng, rng), ylim=(-rng, rng), zlim=(-2*zl, zl), xlabel='$x_1$', ylabel='$x_2$', zlabel='$p(x)$')\n",
    "    ax.set_xticks(np.arange(-rng, rng+1, 5) )\n",
    "    ax.set_yticks(np.arange(-rng, rng+1, 5) )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e262de1",
   "metadata": {},
   "source": [
    "<font color =\"blue\">Now, try fitting an GMM model using EM with a different number of components, or try to randomise the initialisation differently, try to use both ways provided to deal with singular components, etc. Report your findings.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91400568",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487001c",
   "metadata": {},
   "source": [
    "## 10.3 - Using sklearn\n",
    "\n",
    "Here we explore the SciKit Learn solution to do the same as above. In sklearn, the EM algorithm is integrated in the GMM class' `fit()` function.\n",
    "\n",
    "To compare it to our own implementation, we will initialise it ourselves manually, to the same values as we did with ours (we will always be recalculating ours with the same initial conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b34dfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 4\n",
    "means, covs, coefs = initialise(nComponents)\n",
    "\n",
    "# The sklearn implementation requires precisions instead of covariance matrices (which are just the inverse of the covariances)\n",
    "precs = np.linalg.inv(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03adbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm_sklearn = GaussianMixture(n_components=nComponents, weights_init = coefs, means_init = means, precisions_init = precs, max_iter = 100).fit(data)\n",
    "print(gmm_sklearn.means_)\n",
    "print(gmm_sklearn.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c21361",
   "metadata": {},
   "source": [
    "Let's visualise the results of sklearn.\n",
    "\n",
    "*NOTE: There is no equivalent to our `pdf()` function... Instead, the function that we can use here is called `GaussianMixture.score_samples()` and returns the log likelihood, while we want to plot the likelihood. So, to compare equal things we we need to exponantiate this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "043946da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.exp(gmm_sklearn.score_samples(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f96a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(f, X, gmm_sklearn.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759dee2",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Compare these results to our implementation, to make sure our implementation works well (make sure you are using the same initial values).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34c8bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5234f",
   "metadata": {},
   "source": [
    "# 10.4 - Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2fc23",
   "metadata": {},
   "source": [
    "The original dataset also had class information. Therefore we can calculate the likelihood (class conditional probability of x) for each class, as well as the prior of each class, and then use it to predict posterior probabilities for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3a23c",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Calculate the prior probabilities of the two classes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da78d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e3333",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Calculate the class likelihoods (class conditioned probability density functions) for the two classes, using EM</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f60496c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fd646",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Now apply Bayes, and calculate the posterior probabilities for each class for points [-3, 2], [7, 0], [1,-6], [5, -5]. To which class would you classify each point?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4c3fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5b37174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (Cont.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b119f",
   "metadata": {},
   "source": [
    "<font color = \"blue\">Make a function to classify a point using the Bayes rule.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4c29913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078d726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
