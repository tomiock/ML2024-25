{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Feature Extraction\n",
    "\n",
    "In this notebook we are going to extract Bag of Words (BoW) features from a dataset of image patches that correspond to either text or non-text areas in an image.\n",
    "\n",
    "The roadmap is as follows:\n",
    "\n",
    "* Import train/test data (raw image pixels). We use the same dataset as in PR1, raw_pixels_dataset_5980.pklz\n",
    "* Extract small image patches from your training set\n",
    "* Use K-Means to learn a vocabulary of Visual Words\n",
    "* Represent the train images as BoW histograms\n",
    "* Represent the test images as BoW histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usupervised Learning of Visual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset (Raw Pixels data)\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with gzip.open('./raw_pixels_dataset_5980.pklz','rb') as f:\n",
    "    (train_labels,train_images,test_labels,test_images) = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(\"Train_images shape \" + str(train_images.shape))\n",
    "print(\"Test_images shape  \" + str(test_images.shape))\n",
    "\n",
    "# Show a few samples of the positive and negative classes.\n",
    "num_text = sum(train_labels==0)\n",
    "fig = plt.figure()\n",
    "for i in range(1,6):\n",
    "    ax = fig.add_subplot(2, 5, i)\n",
    "    ax.imshow(np.reshape(train_images[np.random.randint(0,num_text),:],[32,32]), cmap=plt.cm.gray)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = fig.add_subplot(2, 5, i+5)\n",
    "    ax.imshow(np.reshape(train_images[np.random.randint(num_text,train_labels.shape[0]),:],[32,32]), cmap=plt.cm.gray)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract patches (of 8x8 pixels) from train images\n",
    "import numpy as np\n",
    "\n",
    "images = train_images.reshape((-1,32,32)).astype('float32')/255 # reshape images to 32x32\n",
    "                                                                # and scale values to (0., 1.)\n",
    "print(\"Images shape \" + str(images.shape))\n",
    "\n",
    "# Collect image patches with sliding window (8x8) in each train image sample\n",
    "PATCH_SIZE=8;\n",
    "STEP_SIZE =8;\n",
    "\n",
    "patches = np.zeros((0,PATCH_SIZE,PATCH_SIZE))\n",
    "\n",
    "for x in range(0, 32-PATCH_SIZE+1, STEP_SIZE):\n",
    "    for y in range(0, 32-PATCH_SIZE+1, STEP_SIZE):\n",
    "        patches = np.concatenate((patches, images[:,x:x+PATCH_SIZE,y:y+PATCH_SIZE]), axis=0)\n",
    "\n",
    "patches = patches.reshape((patches.shape[0],-1))\n",
    "print(\"Patches shape \" + str(patches.shape))\n",
    "\n",
    "# Visualize a few patches\n",
    "fig = plt.figure()\n",
    "for i in range(1,6):\n",
    "    ax = fig.add_subplot(1, 5, i)\n",
    "    ax.imshow(np.reshape(patches[i],[PATCH_SIZE,PATCH_SIZE]), cmap=plt.cm.gray, vmin=0, vmax=1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we contrast normalize our patches in order to gain illumination invariance. Each patch is normalized by pixel-wise subtracting its mean and dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PreProcessing (I) Contrast Normalization of Patches\n",
    "\n",
    "mu = patches.mean(axis=1) # mean values\n",
    "sigma = patches.std(axis=1) + np.ptp(patches, axis=1)/20.0 # standard deviation (plus a small value)\n",
    "\n",
    "patches = (patches-mu.reshape([-1,1]))/(sigma.reshape([-1,1])) # subtract the mean and divide by std\n",
    "\n",
    "# Set NaN values (if exist) to 0\n",
    "w = np.isnan(patches);\n",
    "patches[w] = 0;\n",
    "\n",
    "# Visualize a few Constrast Normalized patches\n",
    "fig = plt.figure()\n",
    "for i in range(1,6):\n",
    "    ax = fig.add_subplot(1, 5, i)\n",
    "    ax.imshow(np.reshape(patches[i],[PATCH_SIZE,PATCH_SIZE]), cmap=plt.cm.gray)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform ZCA whitening of the patches. \n",
    "\n",
    "We know how to use PCA to reduce the dimensionality of the data. ZCA whitening is a closely related preprocessing step. \n",
    "\n",
    "When we are working with images the raw input is redundant, because the values of adjacent pixels in an image are highly correlated. The goal of whitening is to decorrelate pixel values with each other and make them all have the same variance.\n",
    "\n",
    "ZCA whitening is implemented as follows:\n",
    "\n",
    "* First, subtract the mean value of each pixel in image patches to make our data zero-mean.\n",
    "* Then, as in PCA, compute the eigenvectors of the patches' covariance matrix $\\Sigma$.\n",
    "* The ZCA whitened data is then : \n",
    "\n",
    "$x_{ZCAwhite} = U * diag(1./\\sqrt{diag(S) + \\epsilon)} * U' * x$\n",
    "\n",
    "where $U$ contains the eigenvectors of $\\Sigma$, $S$ contains the corresponding eigenvalues, $\\epsilon$ is a trivial small value and $x$ is the input data (image patches).\n",
    "\n",
    "More info: http://ufldl.stanford.edu/wiki/index.php/Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PreProcessing (II) ZCA Whitening of normalized patches\n",
    "\n",
    "eig_values, eig_vec = np.linalg.eig(np.cov(patches.T))\n",
    "epsilon = 0.01\n",
    "pca = eig_vec.dot(np.diag((eig_values+epsilon)**-0.5).dot(eig_vec.T))\n",
    "\n",
    "\n",
    "M =  patches.mean(axis=0)\n",
    "patches = patches -  M # subtract average value\n",
    "patches = np.dot(patches, pca) # perform pca whitening\n",
    "\n",
    "\n",
    "# Visualize a few PreProcessed patches\n",
    "fig = plt.figure()\n",
    "for i in range(1,6):\n",
    "    ax = fig.add_subplot(1, 5, i)\n",
    "    ax.imshow(np.reshape(patches[i],[PATCH_SIZE,PATCH_SIZE]), cmap=plt.cm.gray)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our preprocessed training patches we can perform Unsupervised Learning analysis (K-means clustering) to learn the K visual Words vocabulary.  Then we'll use these vocabulary to represent an image as an histogram of the occurrences of its Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# K-means clustering to learn K visual_words from data\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_VISUAL_WORDS = 64\n",
    "\n",
    "km = KMeans(n_clusters=NUM_VISUAL_WORDS, max_iter=50, n_init=1, verbose=False)\n",
    "km.fit(patches)\n",
    "\n",
    "visual_words = km.cluster_centers_\n",
    "print(\"Visual_words shape \" + str(visual_words.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned vocabulary of Visual Words\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#%pylab inline\n",
    "\n",
    "fig = plt.figure()\n",
    "num_col = int(np.ceil(float(NUM_VISUAL_WORDS)/4))\n",
    "for i in range(NUM_VISUAL_WORDS):\n",
    "    ax = fig.add_subplot(4, num_col, i+1)\n",
    "    visual_word_ = visual_words[i,:]\n",
    "    visual_word_ = visual_word_.reshape(PATCH_SIZE,PATCH_SIZE);\n",
    "    ax.imshow(visual_word_, interpolation='none', cmap = cm.Greys_r)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image features using Bag of Words\n",
    "\n",
    "A Bag of Visual Words is a histogram of occurrence counts of the vocabulary words.\n",
    "\n",
    "In our case we have 64 Words in the vocabulary, hence the histogram will have 64 bins. \n",
    "\n",
    "To represent an image we'll extract image patches, preprocess them, and find its Nearest Neighbor Word in the vocabulary. Finally, each Nearest Neighbor found increments in 1 unit the value of its respective bin in the BoW histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learn a KNN classifier, Each visual Word represents one class\n",
    "# In this case KNN is used simply as a way to search of the nearest neighbor Visual Word \n",
    "# in our vocabulary.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neig = KNeighborsClassifier(n_neighbors=1)\n",
    "neig.fit(visual_words,range(0,NUM_VISUAL_WORDS));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from train images\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_features = np.zeros((images.shape[0],visual_words.shape[0]));\n",
    "\n",
    "for i in tqdm(range(0,images.shape[0])): # for each image\n",
    "\n",
    "  # Do sliding window (8x8) in each image to extract patches\n",
    "  # then normalize, whiten and build the Bag of Words histogram\n",
    "    for x in range(0,32-PATCH_SIZE+1,STEP_SIZE):\n",
    "        for y in range(0,32-PATCH_SIZE+1,STEP_SIZE):\n",
    "            patch = images[i,x:x+PATCH_SIZE,y:y+PATCH_SIZE]\n",
    "            patch = patch.reshape((1,-1))\n",
    "            # PreProcessing (I): Normalize\n",
    "            mu = patch.mean(axis=1) # mean values\n",
    "            sigma = patch.std(axis=1) + max(np.ptp(patch, axis=1)/20.0, 0.0001) # standard deviation \n",
    "                                                                   # (plus a small value)\n",
    "            patch = (patch-(mu[np.newaxis,:]).T)/(sigma[np.newaxis,:]).T\n",
    "            # Set NaN values to 0\n",
    "            w = np.isnan(patch);\n",
    "            patch[w] = 0;\n",
    "\n",
    "            # PreProcessing (II): ZCA whitening\n",
    "            patch = patch - M # subtract average value\n",
    "            patch = np.dot(patch, pca) # perform pca whitening\n",
    "\n",
    "            # BoW\n",
    "            nn = neig.predict(patch)\n",
    "            train_features[i,nn] = train_features[i,nn] + 1;\n",
    "        \n",
    "    train_features[i,:] = train_features[i,:]/max(train_features[i,:]) # Histogram normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# If you want to save your training features\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('./BoW_train_features.pklz','wb') as f:\n",
    "    pickle.dump((train_labels,train_features),f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from test images\n",
    "images = test_images.reshape((-1,32,32)).astype('float32')/255\n",
    "test_features = np.zeros((images.shape[0],visual_words.shape[0]));\n",
    "\n",
    "for i in tqdm(range(0,images.shape[0])): #for each image\n",
    "    \n",
    "  # Do sliding window (8x8) in each image to extract patches\n",
    "  # then normalize, whiten and build the Bag of Words histogram\n",
    "    for x in range(0,32-PATCH_SIZE+1,STEP_SIZE):\n",
    "        for y in range(0,32-PATCH_SIZE+1,STEP_SIZE):\n",
    "            patch = images[i,x:x+PATCH_SIZE,y:y+PATCH_SIZE]\n",
    "            patch = patch.reshape((1,-1))\n",
    "            # PreProcessing (I): Normalize\n",
    "            mu = patch.mean(axis=1) # mean values\n",
    "            sigma = patch.std(axis=1) + max(np.ptp(patch, axis=1)/20.0, 0.0001) # standard deviation \n",
    "                                                                   # (plus a small value)\n",
    "            patch = (patch-(mu[np.newaxis,:]).T)/(sigma[np.newaxis,:]).T\n",
    "            # Set NaN values to 0\n",
    "            w = np.isnan(patch);\n",
    "            patch[w] = 0;\n",
    "\n",
    "            # PreProcessing (II): ZCA whitening\n",
    "            patch = patch - M # subtract average value\n",
    "            patch = np.dot(patch, pca) # perform pca whitening\n",
    "\n",
    "            # BoW\n",
    "            nn = neig.predict(patch)\n",
    "            test_features[i,nn] = test_features[i,nn] + 1;\n",
    "        \n",
    "    test_features[i,:] = test_features[i,:]/max(test_features[i,:])  # Histogram normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# If you want to save your test features\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('./BoW_test_features.pklz','wb') as f:\n",
    "    pickle.dump((test_labels,test_features),f,pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
