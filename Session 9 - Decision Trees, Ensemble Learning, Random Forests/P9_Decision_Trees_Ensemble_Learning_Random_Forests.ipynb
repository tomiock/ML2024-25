{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d824c8",
   "metadata": {},
   "source": [
    "# Exercises 8 - Decision Trees, Ensemble Methods, Random Forests \n",
    "This lab session is composed of two main parts:\n",
    "\n",
    "1. We will examine the decision boundaries and overfitting of **Decision Tree** classifiers. Then, we will conduct a **Grid Search** to find optimum hyperparameters for our model. TIP: You will probably want to apply the Grid/Randomized Search to fine-tune your future models.\n",
    "2. We will witness the *wisdom of the crowd* by analysing various **Ensemble Learning** methods. First, we'll start with *Voting Classifier*, *Bagging*, *Pasting*, *Random Forests* for classification. Then, we will move on to regression and we will see how we can boost a base Decision Tree regressor with *AdaBoost* and *Gradient Boosting*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249b76f",
   "metadata": {},
   "source": [
    "We start by importing the usual libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17769078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from typing import List\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f8dee",
   "metadata": {},
   "source": [
    "We will use the same random seed through all exercises for the sake of reproducability of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a88260",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76c392",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0e19b",
   "metadata": {},
   "source": [
    "We will first work with the familiar [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756abe11",
   "metadata": {},
   "source": [
    "Let's choose two features to work with and get the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\" , \"petal width (cm)\"]].to_numpy()\n",
    "y = iris.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c5002",
   "metadata": {},
   "source": [
    "We create Training and Test sets. Note the random state parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f50e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3088ce",
   "metadata": {},
   "source": [
    "## Decision boundaries and accuracy with different tree depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce46fc",
   "metadata": {},
   "source": [
    "We define a function to visualize the decision boundaries for a two-feature dataset.\n",
    "> Just have an idea what the function does, you will only be asked to use it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMAP = plt.cm.tab10  # Color palette\n",
    "\n",
    "def plot_decision_boundaries2D(clf: object, X_train, X_test, y_train, y_test, xlabel=\"X1\", ylabel=\"X2\", \n",
    "                               plot_train=True, plot_test=True, cmap: ListedColormap=CMAP):\n",
    "    \"\"\"Plots the decision boundaries and training and test data points.\"\"\"\n",
    "    n_new_points = 100\n",
    "    X = np.vstack((X_train, X_test))\n",
    "    x1, x2 = X[:, 0], X[:, 1]\n",
    "    x1s = np.linspace(math.floor(x1.min()), math.ceil(x1.max()), n_new_points)\n",
    "    x2s = np.linspace(math.floor(x2.min()), math.ceil(x2.max()), n_new_points)\n",
    "    x1_new, x2_new = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1_new.ravel(), x2_new.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1_new.shape)\n",
    "    \n",
    "    # Plot Training data points\n",
    "    if plot_train:\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=cmap(y_train), alpha=0.3,\n",
    "                    marker=\"o\", label=\"Training\")\n",
    "    # Plot Test data points\n",
    "    if plot_test:\n",
    "        miss = y_test != clf.predict(X_test)\n",
    "        plt.scatter(X_test[~miss][:, 0], X_test[~miss][:, 1], c=cmap(y_test[~miss]), edgecolor=\"k\", \n",
    "                    marker=\"o\", label=\"Test correctly classified\")\n",
    "        plt.scatter(X_test[miss][:, 0], X_test[miss][:, 1], c=cmap(y_test[miss]), edgecolor=\"r\",\n",
    "                    marker=\"D\", label=\"Test misclassified\")\n",
    "    # Plot Decision Boundaries\n",
    "    plt.contourf(x1_new, x2_new, y_pred, alpha=0.05)\n",
    "    # Configure plot\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim([x1s.min(), x1s.max()])\n",
    "    plt.ylim([x2s.min(), x2s.max()])\n",
    "    plt.suptitle(f\"Decision boundaries of the '{clf.__class__.__name__}''\")\n",
    "    plt.title(\"with Training and Test data points\")\n",
    "    if plot_train or plot_test:\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd59b8b",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a [`sklearn.tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to the Iris training data with the following parameters:<br/>- criterion: `\"entropy\"`,<br/>- maximum depth: `1`,<br/>- random state: `RANDOM_SEED`.<br/>You may follow the given link to see how the class is used.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0598d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# tree_iris1 = \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72d8f6",
   "metadata": {},
   "source": [
    "Let's plot the decision boundaries for the **decision stump** we have just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a981e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(tree_iris1, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f442d",
   "metadata": {},
   "source": [
    "Now, we plot the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree(tree_iris1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643a07d",
   "metadata": {},
   "source": [
    "<font color=blue>1. How many nodes and how many leaves do you see? <br/> 2. Interpret the entropy in each leaf, i.e. Why high or why low?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad87e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf9912",
   "metadata": {},
   "source": [
    "<font color=blue>What is the accuracy of the decision stump on the test set?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb57b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# YOUR CODE HERE\n",
    "# accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e25ce2",
   "metadata": {},
   "source": [
    "<font color=blue>Grow another tree with same the training data and parameters above, but this time set the maximum depth to `3`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# tree_iris3 = \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffaa3ac",
   "metadata": {},
   "source": [
    "We plot the decision boundaries of this tree as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a827e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(tree_iris3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b67cc8",
   "metadata": {},
   "source": [
    "Let's plot this tree too just for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree(tree_iris3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8718c",
   "metadata": {},
   "source": [
    "<font color=blue>What is the accuracy of the new tree `tree_iris3` on the test set?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451418c",
   "metadata": {},
   "source": [
    "You should see that the accuracy has increased drastically with a tree of depth=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b6d39",
   "metadata": {},
   "source": [
    "<font color=blue>Now, fit the last tree to the same training data and with the same parameters above but this time set the maximum depth to `5`. Then, calculate its accuracy on the test set.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# tree_iris5 =\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d2221",
   "metadata": {},
   "source": [
    "<font color=blue>Interpret `tree_iris5`'s accuracy with respect to `tree_iris3`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801a41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cea186",
   "metadata": {},
   "source": [
    "## Search for best tree hyperparamaters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ecd7f",
   "metadata": {},
   "source": [
    "*TIP: You can apply the [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) method that we will see here to other ML algorithms to find their hyperparameters. Compare it with the [Randomized Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Sometimes, you may want to use a combination of these two methods. The latter for exploration, the former for exploitation of the hyperparameter space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2db87",
   "metadata": {},
   "source": [
    "We start by creating a new dataset. We use `scikit-learn`'s [`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function that produces two interleaving half circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264de82",
   "metadata": {},
   "source": [
    "We plot the new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2dd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=ListedColormap([\"#0000FF\", \"#FFFF00\"]), edgecolors=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786667f",
   "metadata": {},
   "source": [
    "We define a short function that gives info about a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee00570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_info(tree_):\n",
    "    \"\"\"Print brief insights of the tree.\"\"\"\n",
    "    print(\"depth:\", tree_.get_depth(), \", n_leaves:\", tree_.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e07e6",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a tree to the Moons training data and with no tree limitations. Just set the random state to `RANDOM_SEED` for the sake of reproducibility. Then, calculate the tree's accuracy on the test set.</font>\n",
    "> Note that since we did not specify the impurity criterion, Scikit-Learn will use the default [*Gini Index*](https://scikit-learn.org/stable/modules/tree.html#classification-criteria), i.e., not the Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363572db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# tree_moons = \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adaa51a",
   "metadata": {},
   "source": [
    "Check the depth and number of leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_info(tree_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692e05e",
   "metadata": {},
   "source": [
    "The decision boundaries plot clearly shows that this tree overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(tree_moons, X_train, X_test, y_train, y_test, plot_train=False, plot_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ae8dd",
   "metadata": {},
   "source": [
    "Now, we conduct a **grid search** for the best hyperparameters to fit a tree to the above Moons data. We first define the set of possible values for each hyperparameter of interest. For now, we are just interested in limiting the *maximum number of leaves* and *the minimum number of samples required to split an internal node*. We will use 5-fold cross validation for each (parameter 1, parameter 2) pair.\n",
    "> Just try to understand how a Grid Search is conducted. You will not be asked to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4, 5]}\n",
    "grid_moons = GridSearchCV(DecisionTreeClassifier(random_state=RANDOM_SEED), params, verbose=1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ff11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_moons.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625dda1",
   "metadata": {},
   "source": [
    "Now, we can get the best estimator tree found in the search. We see that the optimum tree is much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_moons = grid_moons.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9835b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_info(best_tree_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3fd13",
   "metadata": {},
   "source": [
    "<font color=blue>Calculate the best tree's accuracy on the test set. The accuracy should be better than the unregularized tree.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e2dbd",
   "metadata": {},
   "source": [
    "Decision boundaries plot shows that the best tree generalizes much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa37a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(best_tree_moons, X_train, X_test, y_train, y_test, plot_train=False, plot_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344caff0",
   "metadata": {},
   "source": [
    "We will continue to use the moons training and test sets below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcaec9b",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b2f04",
   "metadata": {},
   "source": [
    "### The Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f43e8",
   "metadata": {},
   "source": [
    "<font color=\"blue\">You have a biased coin with 51% chance of coming up heads. What are the probabilities (with at least a precision of two decimals) of having a **majority of heads** in 1000 and 10000 independent tosses, respectively?<br/>\n",
    "HINT: Remember that these were given as ~73% and ~98% in the lecture slides and remember also that the number of heads in consecutive (independent) tosses of the same coin follows the [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution). You can use the online binomial probability calculator [here](https://stattrek.com/online-calculator/binomial) or calculate it yourself with the [`scipy.stats.binom`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fbe29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18e860",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d776c",
   "metadata": {},
   "source": [
    "We will fit a [`sklearn.ensemble.VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) that uses the votes of an trio ensemble of Decision Tree, Logistic Regression and Na√Øve Bayes classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_nb = GaussianNB()\n",
    "clf_tree = DecisionTreeClassifier(random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa89b2",
   "metadata": {},
   "source": [
    "We first fit the Voting Classifier with *hard voting*, i.e. Majority Vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22055cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_voting_hard = VotingClassifier(\n",
    "    estimators=[('lr', clf_logreg), ('nb', clf_nb), ('dt', clf_tree)],\n",
    "    voting='hard')\n",
    "clf_voting_hard.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8a940",
   "metadata": {},
   "source": [
    "We define a simple function to compare accuracies of different classifiers on the same data, then we call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifiers_accuracies(clfs: List[object], X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fit each classifier and compare their accuracies.\"\"\"\n",
    "    for clf in clfs:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(f\"{clf.__class__.__name__}:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_classifiers_accuracies([clf_logreg, clf_nb, clf_tree, clf_voting_hard], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132a876",
   "metadata": {},
   "source": [
    "This time, we did not have much luck and the Voting Classifier could not surpass the individual classifiers of this ensemble with the *Majority Vote*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e7d57",
   "metadata": {},
   "source": [
    "<font color=blue>Try an ensemble of the same classifiers with *soft voting* instead.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# clf_voting_soft =\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a24d5c",
   "metadata": {},
   "source": [
    "Let's compare all again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_classifiers_accuracies([clf_logreg, clf_nb, clf_tree, clf_voting_soft], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35405fd",
   "metadata": {},
   "source": [
    "<font color=blue>1. What is the new accuracy of the ensemble? <br/>2. Interpret the difference, if any.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df81c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fcfff",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2d1bf",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to the Moons training data above with the following parameters:<br/>- Use `100` Decision Tree classifiers;<br/>- Use `200` samples to train each tree (i.e. 25% of the original training set);<br/>- Use bagging, not pasting;<br/>- Set out-of-bag-evaluation to `True` for validation purpose;<br/>- Set `n_jobs=-1` to use all available processors;<br/>- Set the random state of the BaggingClassfier to `RANDOM_SEED`.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# cls_bagging =\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32257b81",
   "metadata": {},
   "source": [
    "Let's see the out-of-bag evaluation score. Remember from the lecture that this score is kind of a validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bagging.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336ea20",
   "metadata": {},
   "source": [
    "<font color=blue>Calculate the accuracy of this ensemble on the test set. Observe that the test score is close to the validation (OOB) score.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aae229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037b5cd",
   "metadata": {},
   "source": [
    "<font color=blue>Compare the accuracy of the Bagging Ensemble to the single `best_tree_moons`'s accuracy you calculated above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a4caf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec8a4c",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63926b",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to the Moons training data above with the following parameters:<br/>- Grow `100` trees in the forest;<br/>- Use bagging, not pasting;<br/>- Set `n_jobs=-1` to use all available processors;<br/>- Set the random state of the forest to `RANDOM_SEED`.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# clf_rndbag = \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d46a0",
   "metadata": {},
   "source": [
    "<font color=blue>As usual, calculate the accuracy of this random forest on the test set. Observe that it should not be very different from the above Bagging Ensemble of trees. This is because Random Forest classifier is an (optimized) implementation of a Bagging classifier with trees.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b547f1",
   "metadata": {},
   "source": [
    "<font color=blue>Now, fit a random forest with the same parameters as the above forest, but this time use *pasting* instead of bagging. And calculate its accuracy on the test set.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f99de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# clf_rndpaste = \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c25509",
   "metadata": {},
   "source": [
    "You should see that with pasting the accuracy was lower than the one with bagging. That said, the Random Forest with *pasting* still outperforms the single `tree_moons`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd355c",
   "metadata": {},
   "source": [
    "We plot the decision boundaries of the *bagging* and *pasting* ensembles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(clf_rndbag, X_train, X_test, y_train, y_test, plot_train=False, plot_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries2D(clf_rndpaste, X_train, X_test, y_train, y_test, plot_train=False, plot_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0ad5b",
   "metadata": {},
   "source": [
    "<font color=blue>Which of the two methods has a lower variance in this case? Bagging or pasting?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5022be7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e454a",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc156b",
   "metadata": {},
   "source": [
    "We generate a new dataset for the remaining part of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sinusoidal(random_state=None, max_x = 10., n_samples=200, noise=0.1, plot_=True):\n",
    "    \"\"\"Create a 1D sinusoidal dataset. Returns X, y.\"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = np.sort(max_x * rng.rand(n_samples, 1), axis=0)\n",
    "    y = np.sin(X).ravel() + noise * rng.randn(n_samples)\n",
    "    if plot_:\n",
    "        plt.scatter(X, y, color=\"darkorange\", marker=\".\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(f\"n_samples:{n_samples}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_sinusoidal(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc74ea",
   "metadata": {},
   "source": [
    "And, we create our new train and tests sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14289fb3",
   "metadata": {},
   "source": [
    "We define a function for the plots below that you will use as-is. See how the [Mean Absolute Errors](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAEs) are calculated w.r.t the true function and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(true_f, fit_predictor, X_train, X_test, y_train, y_test, n_points=100):\n",
    "    \"\"\"Plots the true underlying function, training data and predictions and computes MAEs.\"\"\"\n",
    "    # Prediction points for plotting only\n",
    "    X_plot = np.linspace(X_train.min(), X_train.max(), n_points)[:, np.newaxis]\n",
    "    # Training data\n",
    "    plt.scatter(X_train, y_train, color=\"darkorange\", marker=\".\", label=\"training data\")\n",
    "    # True underlying function\n",
    "    plt.plot(X_plot, true_f(X_plot), color=\"red\", linewidth=2, linestyle=\"-\", alpha=0.7, label=\"true function\")\n",
    "    # Predictions for the X_plot\n",
    "    plt.plot(X_plot, fit_predictor.predict(X_plot), color=\"royalblue\", linewidth=2, linestyle=\"-\", label=\"prediction\")\n",
    "    # Mean Absolute Errors\n",
    "    y_true = true_f(X_test).reshape(-1)\n",
    "    y_pred = fit_predictor.predict(X_test)\n",
    "    mae_true = np.mean(np.abs(y_true - y_pred))  # MAE on true function\n",
    "    mae_test = np.mean(np.abs(y_test - y_pred))  # MAE on test set\n",
    "    plt.title(f\"MAE True:{mae_true:.4f}, MAE Test:{mae_test:.4f}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cdb5f",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a Regression Tree to the sinusosidal data. Set the tree's maximum depth to 5 and random state to `RANDOM_SEED`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# reg_tree5 =\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a2808",
   "metadata": {},
   "source": [
    "Let's plot the predictions of the tree. Observe that a tree of depth 5 more or less fits the data, i.e., it is neither overfitting nor too underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(np.sin, reg_tree5, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca73a2",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec681a8",
   "metadata": {},
   "source": [
    "Now, we will use the [`sklearn.ensemble.AdaBoostRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) regressor to boost our single tree and see if we can have an ensemble that generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956becb",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a `AdaBoostRegressor` to the sinusoidal training data above with the following parameters:<br/>- Use `20` Regression Trees with maximum depth of `5`;<br/>- Set learning rate to `1`<br/>- Set the random state to `RANDOM_SEED`.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# reg_ada5 = \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1975363",
   "metadata": {},
   "source": [
    "Let's see how the boosted ensembele performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a46373",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(np.sin, reg_ada5, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079808ec",
   "metadata": {},
   "source": [
    "<font color=blue>Interpret the Mean Absolute Errors of the single Decision Tree (`reg_tree5`) and AdaBoost (`reg_ada5`) regressors.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d343271",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*YOUR RESPONSE HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e8031",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a76b3",
   "metadata": {},
   "source": [
    "We start with implementing our custom Gradient Boosting Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cd791",
   "metadata": {},
   "source": [
    "<font color=blue>Fill in the indicated three lines with your code. Each line should end up as a single line.<br/>HINT: See the pseudocode in the lecture slides.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientTreeBoostingRegressor:\n",
    "    \"\"\"Custom Gradient Boosting Regressor with Decision Tree as the base estimator.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1, random_state=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.M = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.hypotheses = []  # i.e., trees\n",
    "        \n",
    "    def fit(self, X, y):  \n",
    "        \n",
    "        self.F_0 =  # YOUR CODE HERE\n",
    "        F_m = self.F_0\n",
    "        \n",
    "        for m in range(self.M):\n",
    "            r_m = y - F_m\n",
    "            h_m = DecisionTreeRegressor(max_depth=self.max_depth, random_state=self.random_state)\n",
    "            h_m.fit(X, r_m)\n",
    "            gamma_m = h_m.predict(X)\n",
    "            F_m = F_m  + self.learning_rate *  # YOUR CODE HERE\n",
    "            self.hypotheses.append(h_m)\n",
    "            \n",
    "    def predict(self, X): \n",
    "        \n",
    "        F_m = self.F_0\n",
    "        \n",
    "        for m in range(self.M):\n",
    "            F_m = F_m  + self.learning_rate *  # YOUR CODE HERE\n",
    "            \n",
    "        return F_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77334e3b",
   "metadata": {},
   "source": [
    "We fit our Gradient Booster to the sinusoidal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_my_gtb = MyGradientTreeBoostingRegressor(\n",
    "    n_estimators=1000, \n",
    "    max_depth=1,\n",
    "    learning_rate=0.1, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "reg_my_gtb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(np.sin, reg_my_gtb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504c9d4",
   "metadata": {},
   "source": [
    "<font color=blue>Fit a [`sklearn.ensemble.GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) to the sinusoidal training data above with the same parameters we used initiating our own MyGradientTreeBoostingRegressor. Then, plot the predictions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# reg_sklearn_gtb = \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d14785",
   "metadata": {},
   "source": [
    "OMG! Our own Gradient Boosting implementation and scikit-learn's yielded identical results! (At least, they should have!) Congratulations! üëè <br/>\n",
    "Note that with only decision stumps&mdash;well, with a thousand of them&mdash;we have been able to generalize almost perfectly despite the noise in the data! This is pretty amazing, isn't it?üòÉ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uab",
   "language": "python",
   "name": "uab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
